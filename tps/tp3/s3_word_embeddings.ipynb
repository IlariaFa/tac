{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings : le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### telecharger les stops words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ilaria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Télécharger stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Stopwords supplémentaires\n",
    "extra_sw = {\n",
    "    \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "    \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "    \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "    \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "    \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"etxc\", \"elles\", \"dés\",\n",
    "    \"prixx\", \"écr\", \"géné\", \"app\", \"adr\", \"mod\", \"bur\", \"trav\", \"et\", \"de\", \"à\", \"en\",\n",
    "    \"le\", \"la\", \"du\", \"des\", \"aux\", \"un\", \"une\"\n",
    "}\n",
    "sw |= extra_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction nettoyage de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_string(text):\n",
    "    # Supprimer caractères non alphabétiques\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ÿ\\s]\", \" \", text)\n",
    "    # Enlever les répétitions de lettres aberrantes (ex: iiiii, llll)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "    # Minuscules\n",
    "    text = text.lower()\n",
    "    # Tokenisation simple\n",
    "    words = nltk.wordpunct_tokenize(text)\n",
    "    # Filtrage avancé\n",
    "    kept = [\n",
    "        w for w in words\n",
    "        if (\n",
    "            len(w) > 2 and           # minimum 3 lettres\n",
    "            len(w) < 20 and          # éviter les tokens trop longs (\"ltshdyyysnkjdlqj\")\n",
    "            w.isalpha() and\n",
    "            w not in sw and\n",
    "            not w.startswith((\"ij\", \"vv\", \"qq\")) and  # motifs OCR fréquents\n",
    "            not re.match(r\".*[0-9].*\", w)             # pas de chiffres\n",
    "        )\n",
    "    ]\n",
    "    return \" \".join(kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture du texte nettoyé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases nettoyées : 891576\n",
      "['imnri hmu marché tenu hors villa', 'vaain téicj races indigènes', 'rasa îichakdui taureaux iallsènas', 'hollandais dufr', 'îdto vachei laitières vante']\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers ton fichier\n",
    "infile = \"../../data/sents.txt\"\n",
    "\n",
    "# Lecture de toutes les lignes\n",
    "with open(infile, encoding=\"utf-8\") as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "# Nettoyage\n",
    "cleaned_texts = [clean_text_string(text) for text in texts]\n",
    "# On enlève les lignes vides après nettoyage\n",
    "cleaned_texts = [t for t in cleaned_texts if t.strip() != \"\"]\n",
    "\n",
    "print(f\"Nombre de phrases nettoyées : {len(cleaned_texts)}\")\n",
    "print(cleaned_texts[:5])  # afficher les 5 premières phrases pour vérification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ilaria/Desktop/STIC/traitement_auto_corpus/tac/tps/tp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4310706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1q\n"
     ]
    }
   ],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'instance de `Phrases` a été entraînée, elle peut concaténer les bigrams dans les phrases lorsque c'est pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `Phraser` est un objet qui convertit certains unigrams d'une liste en bigrams lorsqu'ils ont été identifiés comme pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous répétons l'opération en envoyant cette fois la liste de bigrams afin d'extraire les trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mi', 'imnri', 'r', 'i', '<<', 'i', 'i', 'hmu', \"'\", 'i', '/', 'tx', \"-'\", 'l', ':', 'marche', 'tenu', 'hors', 'villa', ',', 'la', '9', '.'], ['--', 'u', 'a', 'ete', 'vaain', 'si', 'teicj', '>>', 'm', 'races', 'indigenes', 'de', 'fr', '.'], ['31', '<)', 'a', '5s', \"'\", 'k', '131', 'de', '.'], ['rasa', 'iichakdui', \"'\", 'te', ',', 'do', '(', 'r', '.', '3s0', 'h', '710', '.', 'taureaux', 'iallsenas', ',>', 'ia', 'u', '\\\\', '--', 'a', '--', ';', '0ii', '.'], ['hollandais', ',', 'dufr', '.'], ['0', '.'], ['--', 'a', '9', '.--', 'la', 'idto', '-', 'vachei', 'laitieres', ':', 'bn', 'vante', '1q', '.'], ['vendues', '3', '\\\\', 'au', 'prix', 'la', '410', 'a', '*', '<<', 'i', 'h', '\\\\;', 'genisses', ',', 'kl', '.'], [\"'.\", '9', '.'], ['i', 'l', '.', '2', 'i', '.', 'id', '.'], ['da', '370', 'i', '6lutr', '.'], ['marche', 'a', '<', 'u', 'porcs', '.'], ['--', 'categorie', 'de', 'lt', 'ilashtya', ':', \"'\", '237', 'on', 'vente', ';', 'vendus', '1', 'm', '.', 'do', \"'\", '2', 'i', '.--', 'a', ';:,', 'l', '--;', 'i', '.', 'l', '.'], ['des', 't', \"'\", 'innlrov', '-', 'i3ie', ';>>', 'vente', ',', 'vendus', '93', '.', 'de', '32', '.--', 'a', '52', '.--.'], ['xumiir', '.'], ['10', 'a', '*', 'v', '.--_froment', '>>', 'las', '190', 'kit', '..', 'fr', '.'], ['15', '.', '23', ';', 'mi', \"'\", 'te', '>>,', '--;', 'epautre', '.', 'l3', '.--', ';_seigle', '.'], ['14', '.', '5', '>;', 'avoine', '.'], ['10', '.', '30', ';_orge', '.'], ['--;', 'feveroles', ',', '22', '.--;_pommes', 'do', 'terre', '.'], ['g', '.--;', 'paille', ',', 'v', '.', '--;', 'fcin', '.', '6', '.', '50', ':', 'lioumon', ',', '--.'], ['2', \"%'\", 'ivellom', '.'], ['10', 'mv', '.'], ['--_froment', ',', 'las', '109', 'kll_.,', '(', 'r', '.', '13', '.'], ['--', '\\\\', '--;', 'selgle', '.', 'ii', '.-;\\\\', '--;_avoine', ',', '16', '.', '509', '-;', 'orge', ',', '17', '30', 'j', '--,--;', 'escourgeon', '.'], ['#', 'inint', '-', 't', '>>*', 'o', '<<', 'd', ',', '10', 'nov', '.', '--_froment', 'je', 'culture', ',', 'par', '400kil', '.,', 'fr', '.'], ['10', '.--', '1', '--.--;', 'm', '.', 'commercial', ',', '16', '.'], ['--', 'a', '--.--,', 'seigle', '.'], ['13', '.', '50', 'a', '--;_avoine', '.'], ['10', '.', '50', 'a', '--.--;', 'orge', '.'], ['17', '.--', 'a', '--;', 'farine', 'ira', 'de', 'froment', ',', '0', '.--', 'a', '0', '.'], ['--;', 'son', 'gros', 'de', 'tco', '*', 'ment', ',', '15', '.--', 'a', '--.--.'], ['huiie', 'de', 'colza', ',', 'par', 'hcctoi', '.,', '--', 'a', '--;', 'u', '.', 'epures', ',', '--~', 'a', ';', 'huile', 'de', 'lin', '.'], ['--,--', 'a', '--;', 'tourteaux', 'de', 'colza', 'par', '100_kll', '.,', '--', 'a', '--;_id', '.'], ['de', 'lin', ',', '--', 'a', 'i', 'pommes', 'de', 'terre', 'blanches', ',', '6', '.', '3', '>>', 'a', '0_.--.'], ['...', ',,_,,,,', 'paille', ',', 'fi', '.', '50', 'a', '0_.--;', 'loln', \",'\", '5', '.', '5i', ')', 'a', '0_.--;', 'beurre', ',', 'le_kilo', '.'], ['2', '.', 'o', 'a', '0_.--;', 'oeufs', ',', 'les', '26', ',', '3', '.', '30', 'a', '0_.--;', 'genievre', ',', 'l', \"'\", 'uect', '.,', '--', 'a', 'esprit', ',', '0', '.--', 'a', '0_.--.'], ['tournai', ',', '10', 'nov', '.', '--_froment', 'blaze', ',', 'l', \"'\", 'iiaetoluri', ',', 'fr', '.'], ['--.--', 'a', '--;', 'froment', ',--', 'u', '--;', 'metell', ',', '--', 'a', '--;_seigle', ',--.--', 'a', '--;_avoine', ',', '--', 'a', '--;', 'feve', '-', 'roles', ',', '--.--', 'a', '--;', 'beurre', ',', 'le_kilo', ',', '2', '.', '00', 'a', '3', ',', '10', ';', 'oeufs', ',', 'les', '26', ',', '3', '.', '70', 'u3', '.', 'c0', '.'], ['wacrofiliom', ',', '10', 'nov', '.--', 'beurre', '.', 'le', '1_/', '3', 'il', '-,', '1', '.', '5qa', '1', '.', '60', ',', 'oeufs', ',', 'les', '26', ',', '3', '.', '99', 'a', '1', '.', '16', ';', 'lin', ',', 'les', '100_kilos', ',', '93', '.--', 'a', '145', '.--;', 'etoupes', ',', 'lee', '1u0', 'kii', '.,', '30', '.--', 'a', '35', '.--;_pommes', 'de', 'terre', ',', 'les', '100', '>', 'il', '.,', '3', '.--', 'a', '3', '.', '50', ';', 'jeune', 'porc', ',', 'la_piece', ',', '33', '.--', 'a', '50', '.--;', 'lapin', '.'], ['2', '.--', 'a', '3', '.', '95', ';', 'poulet', ',', '2', '.', '25', 'a', '3', '.'], ['--.'], ['vin', '-', 'e', '<<,', '10', 'nov', '.', '--_froment', ',', 'les', '10j', 'kll_.,', 'fr', '.'], ['18', '.--', 'a', '19', '.', '59', ';_seigle', ',', '16', '.-', 'a', '16', '.', '75', ';_avoine', ',', '16', '.--', 'a', '16', '.', '75', ':', 'pois', '--', 'a', '--;', 'feveroles', ',', '--.--', 'a', '--.--;', 'pommes', 'de', 'terre', ',', '8', '80', 'a', 'g', '.--*,', 'beurre', ',', 'le_kilo', ',', '2', '.', '90', 'a', '3', '.', '20', ';', 'oeufs', ',', 'le', '(', 'iuart3', '-', 'ron', '.'], ['i', '.', '16', 'a', '4', '.', '63', '.'], ['charmanne', '.'], ['consul_general', 'de', 'belgique', 'a', 'ottawa', '(', 'canada', ').', 'a', 'ele', 'nomme_consul', 'general', 'a', 'bang', 'kok', '(', 'siam', '),', 'avec', 'juridiction', 'sur', 'le', 'siam', 'et', 'les', 'slrails', 'seluements', '.'], ['--', 'm', '.', 'kelels', ',', 'consul', 'de', 'belgique', 'a', 'tien_-_tsin', '(', 'chine', '),', 'a', 'ele', 'nomme_consul', 'a', 'ottawa', '(', 'canada_),', 'avec', 'juridiction', 'sur', 'la_federation', 'canadienne', 'et', 'ja', 'colonie', 'de', 'terre', '-', 'neuve', '.'], ['decoration', 'c', \".'\", 'v', \"'\", 'gae', '.'], ['--', 'la', '.', 'decoration_civique', 'a', 'ele', 'decernee', 'aux', 'agents', 'de', 'l', \"'\", 'administration_des_postes', ';', 'designes_ci', '-', 'apres', ',', 'savoir', ':', 'la_medaille', 'de', 'ire_classe', ':', 'a', 'm', '.', 'billy', '.'], ['facteur_rural', '.'], ['la_medaille', 'de', '2e_classe', ':', 'a', 'm', '.', 'goringy', 'facteur', 'local', '.', '.', 'armee', '.'], ['--', 'le', 'sergent', ',', 'en_conge_illimite', ',', 'baudouin', '\"', 'est_nomme', 'sous', '-', 'lieutenanl', 'payeur', 'de', 'reserve', '.'], ['notariat', '.'], ['--', 'sont_acceptees', 'les', 'demissions', 'de', 'm', '.', 'myin', '.'], ['de', 'ses_fonctions', 'de', 'notaire', 'a', 'la', 'residence', 'd', \"'\", 'anvers', ';', 'do', 'm', '.', 'jadol', ',', 'id', '.'], ['de', 'marche', '.'], ['ecole_militaire', '.'], ['--', 'sont_admis', 'a', 'l', \"'\", 'ecole_militaire', ',', 'en_qualite', 'd', \"'\", 'eleves', 'de', 'la', '57e', 'promotion', 'de', 'l', \"'\", 'infanterie', 'et', 'ao', 'la', '<', \"'\", '*', \"'\", '*', '-', '\"', 'i', 'cavalerie', ',', 'les', 'jeunes_gens', 'dont_les_noms', 'suivent', ':', 'hanon', 'de', 'louvet', ',', 'van', 'sprang', ',', 'deyloo', ',', 'petit', ',', 'champagne', '.'], ['masui', ',', 'gerard', ',', 'vanneste', ',', 'lallemand', ',', 'van', 'iioeeke', \".'\", 'rigano', '.'], ['ilendrickx', ',', 'von', 'glaboke', ',', 'sou', '-', \"'\", 'moy', '.', 'couturieaux', ',', 'lucion', ',', 'mersch', ',', 'iledo', ',', 'iloudmont', ',', 'de', 'heusch', '.'], ['terfve', '.'], ['labio', ',', 'brabant', ',', 'franckx', ',', 'foulon', ',', 'fiahiiifirh', '.'], ['simmi', '\"', 'tfannatvnin', 'fivnv', '^', 'f', 'h', '-.', 'ilu', '/', 'phnin', '.'], ['u', 'gernaert', ',', 'simon', \",'\", 'henncquin', ',', 'fcro', \"'\", 't', ',', 'g', '->', 'ile', ',', 'i', \"'\", 'orjo', 'e', \"'\", 'y', 'hannus', '.', 'noel', ',', 'flanieng', ',', 'bri', '-', 'matchovelette', ',', 'gondry', ',', 'doux', ',', 'vermeuleu', ',', 'giilo', 'gillot', ',', 'boufvin', '.'], ['borremans', '.'], ['academie_royale', 'flamande', 'de', 'langue', 'et', 'de', 'litterature', '.'], ['--', 'l', \"'\", 'election', 'faite_par', 'l', \"'\", 'academie', 'flamande', ',', 'dans', 'sa_seance', 'du', '17_octobre', '1906', ',', 'de', 'm', '.', 'le_docteur', 'hugo', 'verriest', ',', 'a', 'ingoyghem', ',', 'en_qualite', 'de', 'membre_effectif', ',', 'en', '-', 'remplacement', 'de', 'feu', 'm', '.', 'janssens', ',', 'est_approuvee', '.'], ['sapeurs_-', 'pompiers_communaux', 'armes', '.'], ['--', 'm', '.', 'pa', '*', 'njels', ',', 'sous', '-', 'fieu', 'tenant', 'au', 'corps', 'arme', 'de', 'sapeurs_-', 'pom', '-', 'iers', 'communaux', 'de', 'schaerheek', ',', 'est_nomme', 'lieule', '-', 'p', 'u', 'teuant', ',-', 'en_remplacement', 'de', 'm', '.', 'verteneuil', ',', 'decede', '.'], ['enseignement_moyen', '.'], ['--', 'mme', 'schaefer', '-', 'misonne', ',.'], ['.', 'directrice', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'elat', 'pour', 'alles', ',', 'a', 'jumet', ',', 'est', 'dechargee', 'des', 'fonciions', 'de', 'regente', 'd', \"'\", 'economie', 'domes', '!'], ['ique', 'a', 'l', \"'\", 'ecole', '\"', 'moyen', 'ne', 'del', \"'\", 'ktai', 'pour', 'ailes', ',', 'a_la_louviere', '.'], ['--', 'm', '.', 'barzin', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'regent', 'a', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'etal', 'pour', \"'\", 'garcons', ',', 'a', 'spa', ',', 'avec', 'autorisation', 'd', \"'\", 'en', 'conserver', 'le', 'r', 'titre_honorifique', ':', 'il', 'est_admis', 'a', 'faire_valoir_ses_droits', ';', 'a', 'ja', 'pension', '.'], [';', \"'\", '-', 'r', '-', 'm', '.', 'drainer', ',', 'directeur', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole', 'moyenue', 'de', 'l', \"'\", 'etat', 'pour', '.', 'garcons', '^', 'a', 'wavre', ',', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'professeur', 'de', 'gymnastique', ',', 'en', ',', 'partage', ',', 'a', 'l', \"'\", 'ecole_moyenne', 'do', 'l', \"'\", 'etat', 'pour', 'garcons', ',', 'a', 'roeulx', '.'], ['*', 'contributions_directes', ',', 'douanes', 'et_accises', '.'], ['--', 'm', '.'], ['du', '-', '#', 'val', ',', 'receveur_des_contributions_directes', 'et', 'des_accises', 'a', 'deynze', '.', 'est_admis', ',', 'sur', 'sa', 'demande', ',', 'a', 'faire_valoir_ses_droits', 'a', 'la', 'pension', 'de', 'retraite', '.'], ['coinmission', 'tnedicale', 'provinciale', '.'], ['--', 'sont_nommes', 'membres', 'correspondants', 'de', 'la_commission', 'medicale', 'piminciale', 'de', 'namur', ';', 'mm', '.'], ['les', 'doctonrs', 'rolin', ',', 'u', \"'\", 'e', 'fosses', ';', 'lebrun', ',', 'de', 'ligny', ';', 'giliard', ',', 'do', 'ctiampion', ',', 'et', '.'], ['defasse', ',', 'de', 'spj', '\\\\', 'en_remplacement', 'de', 'mm', ',', 'les_docteurs', '.'], ['wery', ',', 'de', 'fosses', ',', 'et', 'fermine', ',', 'do', 'lignv', ',', 'demis', '-', ';', 'sionnairos', ',', 'et', 'de', 'mm', '.'], ['jes', 'docteurs', 'renard', ',', \"'\", 'de', 'champion', 'et', 'dehaybe', ',', 'de', 'spy', ',', 'decedes', '.'], ['arts', ',', 'sciences', 'et', 'lettres', 'declamation', '.', 'et', 'diction', '.'], ['*', '--', 'on', 'nous', 'demande', 'de', 'divers_cotes', 'des', 'renseignements_sur', 'le', 'cours', 'que', 'va_donner', 'm', '.', 'hittemans', '.'], ['nous', 'no', 'croyons_pouvoir', 'mieux', 'faire', 'que', 'd', \"'\", 'engager', 'les_interesses', 'a', 's', \"'\", 'adresser', 'a', 'l', \"'\", 'artiste', ',', 'rue', 'verhulst', ',', '6', ',', 'a', 'uccle', '.'], ['universite_libre', ',', 'rue_des_sols', '.'], ['--', 'demain_lundi', ',', 'a', 'r', 'meures', 'du_soir', ',', 'conference', 'par', 'm', '.', 'lameere', ':', '<<', 'la', 'diamonds', 'fondation', '>>.'], ['universite_populaire', 'd', \"'\", 'euerbeek', '.'], ['--', 'lundi', '12', ',', 'a', '3', 'h', '.', '1_/_2', ',', 'au_local', ',', '4', ',', 'rue', 'do', 'l', \"'\", 'etang', ',', 'inauguration', 'solennelle', '.'], ['discours', 'de', 'm', '.', 'e', '.', 'richard', '.'], ['conference', 'de', 'xi', '.', 'ch', '.', 'buls', '.', 'sujet', ':', '<<-', 'la', 'corse', '>>.'], ['projections_lumineuses', '*', 'universite_populaire', 'de', 'saint_-_josse', ',', '67', ',', 'rue', 'de', 'la_limite', '.'], ['demain_lundi', ',', 'a', '8_h', '.', '1_/', '4', ',', 'eauserio', 'litteraire', ':', '<<', 'maxim', 'gorki', '<<.', 'lectures', '.'], ['vamicale', 'de', 'vecole', 'n_deg', '7', '.'], ['--', 'demain_lundi', ',', 'a', '8_h', '.', 'du_soir', ',', 'dans', 'le', 'preau', 'de', 'l', \"'\", 'ecole', 'n_deg', '7', ',', 'rue_haute', '225', \"'\", 'conference', 'par', 'madame', 'journaux', ':', '<<', 'de', 'la', 'grande', '-', 'chartreuse', 'a', 'ja', 'cote', 'd', \"'\", 'azur', '>>.', 'projections_lumineuses', '.'], ['foyer', 'intellectuel', ',', 's0', ',', 'rue', 'du', 'fort', '.'], ['--', 'demain_lundi', ',', 'a', '8b', ',,', 'm', '.', 'j', '.', 'vincent', ':', '<<', 'la_meteorologie', ':', 'le', 'barometre', '>>.']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Entraînement du modèle 1 avec paramètres : {'vector_size': 50, 'window': 5, 'min_count': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modèle 1 sauvegardé !\n",
      "\n",
      " Entraînement du modèle 2 avec paramètres : {'vector_size': 100, 'window': 10, 'min_count': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modèle 2 sauvegardé !\n",
      "\n",
      " Entraînement du modèle 3 avec paramètres : {'vector_size': 100, 'window': 5, 'min_count': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modèle 3 sauvegardé !\n"
     ]
    }
   ],
   "source": [
    "# Liste de combinaisons de paramètres à tester\n",
    "params_list = [\n",
    "    {\"vector_size\": 50, \"window\": 5, \"min_count\": 5},\n",
    "    {\"vector_size\": 100, \"window\": 10, \"min_count\": 5},\n",
    "    {\"vector_size\": 100, \"window\": 5, \"min_count\": 10},\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for i, params in enumerate(params_list):\n",
    "    print(f\"\\n Entraînement du modèle {i+1} avec paramètres : {params}\")\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        vector_size=params[\"vector_size\"],\n",
    "        window=params[\"window\"],\n",
    "        min_count=params[\"min_count\"],\n",
    "        workers=4,\n",
    "        epochs=5\n",
    "    )\n",
    "    models[f\"model_{i+1}\"] = model\n",
    "    model.save(f\"../../data/word2vec_model_{i+1}.model\")\n",
    "    print(f\" Modèle {i+1} sauvegardé !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration et comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exploration de model_1 ===\n",
      "Similarité(ministre, roi) = 0.453\n",
      "Similarité(homme, femme) = 0.649\n",
      "Similarité(paris, londres) = 0.857\n",
      "\n",
      "Mots les plus proches de 'ministre' :\n",
      "  secretaire           0.855\n",
      "  adjoint              0.814\n",
      "  membre               0.814\n",
      "  president            0.814\n",
      "  en_remplacement      0.797\n",
      "\n",
      "Mots les plus proches de 'guerre' :\n",
      "  la_compagnie         0.824\n",
      "  pekin                0.818\n",
      "  retraite             0.802\n",
      "  la_defense           0.798\n",
      "  les_membres          0.786\n",
      "\n",
      "Mots les plus proches de 'belgique' :\n",
      "  suisse               0.788\n",
      "  francfort            0.764\n",
      "  la_banque            0.763\n",
      "  medecine             0.762\n",
      "  hollande             0.755\n",
      "\n",
      "=== Exploration de model_2 ===\n",
      "Similarité(ministre, roi) = 0.507\n",
      "Similarité(homme, femme) = 0.567\n",
      "Similarité(paris, londres) = 0.780\n",
      "\n",
      "Mots les plus proches de 'ministre' :\n",
      "  secretaire           0.840\n",
      "  secretaire_general   0.803\n",
      "  president            0.803\n",
      "  vice_-_president     0.793\n",
      "  conseiller           0.792\n",
      "\n",
      "Mots les plus proches de 'guerre' :\n",
      "  pekin                0.806\n",
      "  la_guerre            0.804\n",
      "  la_defense           0.784\n",
      "  la_compagnie         0.783\n",
      "  la_garnison          0.753\n",
      "\n",
      "Mots les plus proches de 'belgique' :\n",
      "  nationale            0.790\n",
      "  la_banque            0.750\n",
      "  la_societe           0.740\n",
      "  medecine             0.728\n",
      "  banque               0.711\n",
      "\n",
      "=== Exploration de model_3 ===\n",
      "Similarité(ministre, roi) = 0.517\n",
      "Similarité(homme, femme) = 0.553\n",
      "Similarité(paris, londres) = 0.819\n",
      "\n",
      "Mots les plus proches de 'ministre' :\n",
      "  secretaire           0.827\n",
      "  president            0.786\n",
      "  membre               0.780\n",
      "  vice_-_president     0.779\n",
      "  au_ministere         0.770\n",
      "\n",
      "Mots les plus proches de 'guerre' :\n",
      "  pekin                0.781\n",
      "  la_surete            0.755\n",
      "  la_defense           0.753\n",
      "  la_creation          0.745\n",
      "  la_garnison          0.742\n",
      "\n",
      "Mots les plus proches de 'belgique' :\n",
      "  la_banque            0.738\n",
      "  nationale            0.735\n",
      "  suisse               0.714\n",
      "  medecine             0.697\n",
      "  geneve               0.671\n"
     ]
    }
   ],
   "source": [
    "# Charger et explorer les modèles entraînés\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Exploration de {name} ===\")\n",
    "\n",
    "    # 3 exemples de similarité\n",
    "    pairs = [(\"ministre\", \"roi\"), (\"homme\", \"femme\"), (\"paris\", \"londres\")]\n",
    "    for w1, w2 in pairs:\n",
    "        try:\n",
    "            sim = model.wv.similarity(w1, w2)\n",
    "            print(f\"Similarité({w1}, {w2}) = {sim:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"{w1} ou {w2} absent du vocabulaire.\")\n",
    "\n",
    "    # 3 exemples de recherche de mots proches\n",
    "    terms = [\"ministre\", \"guerre\", \"belgique\"]\n",
    "    for t in terms:\n",
    "        try:\n",
    "            print(f\"\\nMots les plus proches de '{t}' :\")\n",
    "            for word, score in model.wv.most_similar(t, topn=5):\n",
    "                print(f\"  {word:20s} {score:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"{t} absent du vocabulaire.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque\n",
    "\n",
    "Vous voyez ici que l'entrainement du modèle est parallélisé (sur 4 workers).\n",
    "\n",
    "Lors qu'on parallélise l'entrainement du modèle, 4 modèles \"séparés\" sont entrainés sur environ un quart des phrases.\n",
    "\n",
    "Ensuite, les résultats sont agrégés pour ne plus faire qu'un seul modèle.\n",
    "\n",
    "On ne peut prédire quel worker aura quelle phrase, car il y a des aléas lors de la parallélisation (p. ex. un worker qui serait plus lent, etc.).\n",
    "\n",
    "Du coup, les valeurs peuvent varier légèrement d'un entrainement à l'autre.\n",
    "\n",
    "Mais, globalement, les résultats restent cohérents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.02499294e+00,  3.82762738e-02, -8.93529475e-01,  5.47164679e-02,\n",
       "        8.22362840e-01,  2.10928822e+00, -1.01254009e-01, -1.20475852e+00,\n",
       "       -2.56319204e-03,  9.00391281e-01,  8.46700251e-01,  1.33263206e+00,\n",
       "       -6.70013130e-01, -4.77592051e-01, -7.70986155e-02, -1.15629065e+00,\n",
       "        1.88836098e+00,  1.48250294e+00, -1.07860732e+00, -2.31762815e+00,\n",
       "       -1.84084281e-01, -1.25240505e+00,  1.09978080e+00,  5.17830670e-01,\n",
       "       -1.01580024e+00,  1.04826128e+00, -4.53097880e-01,  1.65025902e+00,\n",
       "        1.84959427e-01,  1.40506566e+00, -1.50112414e+00, -4.30370748e-01,\n",
       "        1.90948701e+00, -2.03920007e+00, -8.02095771e-01,  1.41926026e+00,\n",
       "        5.75048566e-01, -9.01740551e-01, -4.59442347e-01, -1.56654096e+00,\n",
       "        7.43895322e-02, -1.87444532e+00,  3.51592004e-01,  7.43975863e-02,\n",
       "       -5.06704509e-01,  5.59601545e-01,  9.45686162e-01, -1.95080650e+00,\n",
       "       -2.91851223e-01, -3.98556888e-02,  8.02248716e-01, -1.46690166e+00,\n",
       "       -6.39869213e-01,  9.50436056e-01, -1.39032018e+00, -1.34811056e+00,\n",
       "        4.51851130e-01, -6.77106082e-02, -7.90948033e-01,  2.33985305e-01,\n",
       "        1.02524900e+00, -9.86554265e-01, -4.27968293e-01,  2.05570269e+00,\n",
       "       -1.57418227e+00, -1.34103000e+00, -9.03708994e-01,  3.11957049e+00,\n",
       "       -2.53003192e+00, -2.44518328e+00,  2.22750258e+00,  2.85395861e-01,\n",
       "        1.12542972e-01, -2.20705724e+00,  1.45058155e+00, -4.04232264e-01,\n",
       "        1.13342136e-01,  9.33292806e-01, -1.25555396e+00,  8.50741327e-01,\n",
       "       -1.80507755e+00,  4.43029881e-01, -6.87480927e-01,  1.43077767e+00,\n",
       "        1.16222727e+00,  1.43400323e+00, -2.15973186e+00, -5.85772872e-01,\n",
       "       -6.47266090e-01, -5.47588289e-01, -1.93437338e+00, -7.88004920e-02,\n",
       "       -2.61991143e+00, -2.29468465e+00,  8.64153624e-01,  3.71303767e-01,\n",
       "       -6.49574101e-01,  8.87460709e-01, -1.03568935e+00,  6.40901402e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"ministre\"]\n",
    "#moins c'est moins proche, plus c'est plur proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.5166741)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('secretaire', 0.8270032405853271),\n",
       " ('president', 0.7864551544189453),\n",
       " ('membre', 0.7797642350196838),\n",
       " ('vice_-_president', 0.7794414758682251),\n",
       " ('au_ministere', 0.7696550488471985),\n",
       " ('rapporteur', 0.7654617428779602),\n",
       " ('ancien_ministre', 0.7648983597755432),\n",
       " ('depute', 0.7584162354469299),\n",
       " ('secretaire_general', 0.7553635835647583),\n",
       " ('adjoint', 0.7516523599624634)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('berlin', 0.6857337951660156), ('la_monnaie', 0.6483603715896606), ('marseille', 0.6317960023880005), ('ce_matin', 0.6298128962516785), ('new_-_york', 0.6292555332183838), ('nice', 0.6158302426338196), ('au_theatre', 0.5965142250061035), ('washington', 0.5771775841712952), ('dimanche', 0.5762718319892883), ('hier_soir', 0.5736894011497498)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))\n",
    "#on retire le concept belgique aux concept paris et londre, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
